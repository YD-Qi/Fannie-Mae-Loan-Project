{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "As always, in dealing with real world data, the first step is to clean up the data. While the single family loan data published by Fannie Mae is relatively clean reading and combining all the files together are time-consuming. Ideally, I would like to read in all the acquisition file then combine them into a single file and do the same thing to performance data. However, the data (especially the performance data) is so huge that this is simply not doable on my computer. So I clean and combine them piece by piece. Below are the code for reading cand cleanning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libs\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "low_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up directory\n",
    "\n",
    "DATA_DIR = '/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/raw_data'\n",
    "PROCESSED_DIR ='/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list column names for the two datasets\n",
    "Acquisition=[\n",
    "           'LoanID',\n",
    "           'Channel',\n",
    "           'SellerName',\n",
    "           'OrInterestRate',\n",
    "           'OrUnpaidPrinc',\n",
    "           'OrLoanTerm',\n",
    "           'Origination',\n",
    "           'FirstPayment',\n",
    "           'OrLTV',\n",
    "           'OrCLTV',\n",
    "           'NumBorrowers',\n",
    "           'DTIRat',\n",
    "           'CreditScore',\n",
    "           'FTHomeBuyer',\n",
    "           'LoanPurpose',\n",
    "           'PropertyType',\n",
    "           'NumUnits',\n",
    "           'OccType',\n",
    "           'PropertyState',\n",
    "           'Zip',\n",
    "           'MortInsPerc',\n",
    "           'ProductType',\n",
    "           'CoCreditScore',\n",
    "           'MortInsType',\n",
    "           'RelocationMortgage'],\n",
    "Performance=[\n",
    "           'LoanID',\n",
    "           'ReportPeriod',\n",
    "           'Servicer',\n",
    "           'CurrInterestRate',\n",
    "           'CAUPB','LoanAge',\n",
    "           'MonthsToMaturity',\n",
    "           'AdMonthsToMaturity',\n",
    "           'MaturityDate',\n",
    "           'MSA',\n",
    "           'CLDS',\n",
    "           'ModFlag',\n",
    "           'ZeroBalCode',\n",
    "           'ZeroBalDate',\n",
    "           'LastInstallDate',\n",
    "           'ForeclosureDate',\n",
    "           'DispositionDate', \n",
    "           'ForceclosureCost', \n",
    "           'PPRC',\n",
    "           'AssetRecCost',\n",
    "           'MHRC',\n",
    "           'Tax',\n",
    "           'NetSaleProceeds',\n",
    "           'CreditEnhProceeds',\n",
    "           'RPMWP',\n",
    "           'OtherForceCloProcd',\n",
    "           'NnonIntr_UPB',\n",
    "           'PrinForgAmount',\n",
    "           'RMWPFlag',\n",
    "           'ForceWriteOffAmount',\n",
    "           'ServicingIndicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the columns from Performance dataset to be kept\n",
    "\n",
    "Performance_Select=[\n",
    "        \"LoanID\",\n",
    "        \"MSA\",\n",
    "        \"ForeclosureDate\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to quickly parse datetime columns\n",
    "\n",
    "def lookup(s):\n",
    "    \"\"\"\n",
    "    This is an extremely fast approach to datetime parsing.\n",
    "    For large data, the same dates are often repeated. Rather than\n",
    "    re-parse these, we store all unique dates, parse them, and\n",
    "    use a lookup to convert all dates.\n",
    "    \"\"\"\n",
    "    dates = {date:pd.to_datetime(date) for date in s.unique()}\n",
    "    return s.map(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define concatenate_1 function to process Acquisition data, \n",
    "# drop rows with missing values, \n",
    "# append data from all year-quarter together\n",
    "\n",
    "def concatenate_1(prefix=\"Acquisition\"):\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    full = []\n",
    "    for f in files:\n",
    "        if not f.startswith(prefix):\n",
    "            continue\n",
    "        data = pd.read_csv(os.path.join(DATA_DIR, f), sep=\"|\", header=None, names=Acquisition, index_col=False)\n",
    "        data = data[Acquisition]\n",
    "        data = data.dropna()\n",
    "        full.append(data)\n",
    "\n",
    "    full = pd.concat(full, axis=0)\n",
    "\n",
    "    full.to_csv(os.path.join(PROCESSED_DIR, \"{}.txt\".format(prefix)), sep=\"|\", header=Acquisition, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define concatenate_1 function to process Acquisition data, \n",
    "# drop rows with missing MSA and ForeclosureDate \n",
    "# append data from all year-quarter together\n",
    "\n",
    "def concatenate_2(prefix=\"Performance\"):\n",
    "    files = os.listdir(DATA_DIR)\n",
    "    full = []\n",
    "    for f in files:\n",
    "        if not f.startswith(prefix):\n",
    "            continue\n",
    "        data = pd.read_csv(os.path.join(DATA_DIR, f), sep=\"|\", header=None, names=Performance, \n",
    "            dtype={'LoanID':'int64',\n",
    "'ReportPeriod':'object',\n",
    "'Servicer':'object',\n",
    "'CurrInterestRate':'float64',\n",
    "'CAUPB':'float64',\n",
    "'LoanAge':'int64',\n",
    "'MonthsToMaturity':'float64',\n",
    "'AdMonthsToMaturity':'float64',\n",
    "'MaturityDate':'object',\n",
    "'MSA':'int64',\n",
    "'CLDS':'object',\n",
    "'ModFlag':'object',\n",
    "'ZeroBalCode':'float64',\n",
    "'ZeroBalDate':'object',\n",
    "'LastInstallDate':'object',\n",
    "'ForeclosureDate':'object',\n",
    "'DispositionDate':'object',\n",
    "'ForceclosureCost':'float64',\n",
    "'PPRC':'float64',\n",
    "'AssetRecCost':'float64',\n",
    "'MHRC':'float64',\n",
    "'Tax':'float64',\n",
    "'NetSaleProceeds':'float64',\n",
    "'CreditEnhProceeds':'float64',\n",
    "'RPMWP':'float64',\n",
    "'OtherForceCloProcd':'float64',\n",
    "'NnonIntr_UPB':'float64',\n",
    "'PrinForgAmount':'float64',\n",
    "'RMWPFlag':'object',\n",
    "'ForceWriteOffAmount':'float64',\n",
    "'ServicingIndicator':'object'}\n",
    "                           ,index_col=False)\n",
    "        data = data[[\n",
    "        \"LoanID\",\n",
    "        \"MSA\",\n",
    "        \"ForeclosureDate\"\n",
    "    ]]\n",
    "        data = data.dropna(subset=['MSA', 'ForeclosureDate'])\n",
    "        full.append(data)\n",
    "\n",
    "    full = pd.concat(full, axis=0)\n",
    "\n",
    "    full.to_csv(os.path.join(PROCESSED_DIR, \"{}.txt\".format(prefix)), sep=\"|\", header=[\n",
    "        \"LoanID\",\n",
    "        \"MSA\",\n",
    "        \"ForeclosureDate\"\n",
    "    ], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define concatenate_3 function to concatenate Acquisition and Performance data over all the years\n",
    "def concatenate_3(prefix=\"Performance\"):\n",
    "    files = os.listdir(PROCESSED_DIR)\n",
    "    full = []\n",
    "    for f in files:\n",
    "        if not f.startswith(prefix):\n",
    "            continue\n",
    "        data = pd.read_csv(os.path.join(PROCESSED_DIR, f), sep=\"|\",index_col=False)\n",
    "        full.append(data)\n",
    "    full = pd.concat(full, axis=0)\n",
    "\n",
    "    full.to_csv(os.path.join(PROCESSED_DIR, \"{}.txt\".format(prefix)), sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use concatenate_1 function to process Acquisition data by year\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    concatenate_1(\"Acquisition_2000\")\n",
    "    concatenate_1(\"Acquisition_2001\")\n",
    "    concatenate_1(\"Acquisition_2002\")\n",
    "    concatenate_1(\"Acquisition_2003\")\n",
    "    concatenate_1(\"Acquisition_2004\")\n",
    "    concatenate_1(\"Acquisition_2005\")\n",
    "    concatenate_1(\"Acquisition_2006\")\n",
    "    concatenate_1(\"Acquisition_2007\")\n",
    "    concatenate_1(\"Acquisition_2008\")\n",
    "    concatenate_1(\"Acquisition_2009\")\n",
    "    concatenate_1(\"Acquisition_2010\")\n",
    "    concatenate_1(\"Acquisition_2011\")\n",
    "    concatenate_1(\"Acquisition_2012\")\n",
    "    concatenate_1(\"Acquisition_2013\")\n",
    "    concatenate_1(\"Acquisition_2014\")\n",
    "    concatenate_1(\"Acquisition_2015\")\n",
    "    concatenate_1(\"Acquisition_2016\")\n",
    "    concatenate_1(\"Acquisition_2017\")\n",
    "    concatenate_1(\"Acquisition_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use concatenate_2 function to process Performance data by year\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    concatenate_2(\"Performance_2000\")\n",
    "    concatenate_2(\"Performance_2001\")\n",
    "    concatenate_2(\"Performance_2002\")\n",
    "    concatenate_2(\"Performance_2003\")\n",
    "    concatenate_2(\"Performance_2004\")\n",
    "    concatenate_2(\"Performance_2005\")\n",
    "    concatenate_2(\"Performance_2006\") \n",
    "    concatenate_2(\"Performance_2007\")  \n",
    "    concatenate_2(\"Performance_2008\")\n",
    "    concatenate_2(\"Performance_2009\")\n",
    "    concatenate_2(\"Performance_2010\")\n",
    "    concatenate_2(\"Performance_2011\")\n",
    "    concatenate_2(\"Performance_2012\")\n",
    "    concatenate_2(\"Performance_2013\")\n",
    "    concatenate_2(\"Performance_2014\") \n",
    "    concatenate_2(\"Performance_2015\") \n",
    "    concatenate_2(\"Performance_2016\") \n",
    "    concatenate_2(\"Performance_2017\")  \n",
    "    concatenate_2(\"Performance_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use concatenate_3 to concatenate Acquisition and Performance data over all the years\n",
    "concatenate_3(\"Performance\")\n",
    "concatenate_3(\"Acquisition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "That was a long way! Finally I was able to have concatenated Acquisition and Performance data over all the years available from Fannie Mae. Next steps are to:\n",
    "\n",
    "* read the Acquisition data and Performance data\n",
    "* Merge them by LoanID\n",
    "* Derive new features\n",
    "* Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "acqusition = pd.read_csv('/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data/Acquisition.txt', sep=\"|\",index_col=False)\n",
    "performance = pd.read_csv('/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data/Performance.txt', sep=\"|\",index_col=False)\n",
    "df=pd.merge(acqusition, performance, on='LoanID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Default']=df['ForeclosureDate']\n",
    "df['Default'].fillna(0, inplace=True)\n",
    "df.loc[df['Default'] != 0, 'Default'] = 1\n",
    "df['Default'] = df['Default'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Origination':'loan_date',\n",
    "                          'FirstPayment':'first_payment_date'}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['loan_date', 'first_payment_date', 'ForeclosureDate']]=df[['loan_date', 'first_payment_date', 'ForeclosureDate']].apply(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_payment_year']=df['first_payment_date'].dt.year\n",
    "df['loan_year']=df['loan_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3194421 entries, 0 to 3194420\n",
      "Data columns (total 30 columns):\n",
      "LoanID                int64\n",
      "Channel               object\n",
      "SellerName            object\n",
      "OrInterestRate        float64\n",
      "OrUnpaidPrinc         int64\n",
      "OrLoanTerm            int64\n",
      "loan_date             datetime64[ns]\n",
      "first_payment_date    datetime64[ns]\n",
      "OrLTV                 float64\n",
      "OrCLTV                float64\n",
      "NumBorrowers          float64\n",
      "DTIRat                float64\n",
      "CreditScore           float64\n",
      "FTHomeBuyer           object\n",
      "LoanPurpose           object\n",
      "PropertyType          object\n",
      "NumUnits              int64\n",
      "OccType               object\n",
      "PropertyState         object\n",
      "Zip                   int64\n",
      "MortInsPerc           float64\n",
      "ProductType           object\n",
      "CoCreditScore         float64\n",
      "MortInsType           float64\n",
      "RelocationMortgage    object\n",
      "MSA                   float64\n",
      "ForeclosureDate       datetime64[ns]\n",
      "Default               int64\n",
      "first_payment_year    int64\n",
      "loan_year             int64\n",
      "dtypes: datetime64[ns](3), float64(10), int64(8), object(9)\n",
      "memory usage: 755.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing numeric and categorical columns seperately\n",
    "* numeric columns: replace missing values with mean\n",
    "* categorical columns: replace missing values with most frequent category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_drop=[\"LoanID\", \"Zip\", \"MSA\", 'first_payment_year', 'loan_year']\n",
    "cat_col_add=['Zip', 'first_payment_year', 'loan_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for numeric columns\n",
    "num_mask = ((df.dtypes == int) | (df.dtypes == float))\n",
    "\n",
    "# Get list of numeric column names\n",
    "num_columns = df.columns[num_mask].tolist()\n",
    "num=df[num_columns]\n",
    "\n",
    "# drop \"categorical\" columns\n",
    "num=num.drop(num_col_drop, axis=1)\n",
    "num=num.fillna(num.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for categorical columns\n",
    "cat_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "cat_columns = df.columns[cat_mask].tolist()\n",
    "\n",
    "cat_columns.extend(cat_col_add)\n",
    "cat=df[cat_columns]\n",
    "cat=cat.astype('category')\n",
    "cat=cat.fillna(cat.mode().iloc[0]) # fill na with most requent category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat num and cat columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([num, cat], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the final data for EDA and ML later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(os.path.join(PROCESSED_DIR, \"Fannie_loans.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
