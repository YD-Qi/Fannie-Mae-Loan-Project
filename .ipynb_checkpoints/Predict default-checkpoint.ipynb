{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "## data preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "## models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "## ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# import catboost as cb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "## model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## model evaluation metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data/Fannie_loans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "OrInterestRate        0\n",
       "OrUnpaidPrinc         0\n",
       "OrLoanTerm            0\n",
       "OrLTV                 0\n",
       "OrCLTV                0\n",
       "NumBorrowers          0\n",
       "DTIRat                0\n",
       "CreditScore           0\n",
       "NumUnits              0\n",
       "MortInsPerc           0\n",
       "CoCreditScore         0\n",
       "MortInsType           0\n",
       "Default               0\n",
       "Channel               0\n",
       "SellerName            0\n",
       "FTHomeBuyer           0\n",
       "LoanPurpose           0\n",
       "PropertyType          0\n",
       "OccType               0\n",
       "PropertyState         0\n",
       "ProductType           0\n",
       "RelocationMortgage    0\n",
       "Zip                   0\n",
       "first_payment_year    0\n",
       "loan_year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016885376097890665"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Default\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrInterestRate</th>\n",
       "      <th>OrUnpaidPrinc</th>\n",
       "      <th>OrLoanTerm</th>\n",
       "      <th>OrLTV</th>\n",
       "      <th>OrCLTV</th>\n",
       "      <th>NumBorrowers</th>\n",
       "      <th>DTIRat</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>NumUnits</th>\n",
       "      <th>MortInsPerc</th>\n",
       "      <th>CoCreditScore</th>\n",
       "      <th>MortInsType</th>\n",
       "      <th>Default</th>\n",
       "      <th>Channel</th>\n",
       "      <th>SellerName</th>\n",
       "      <th>FTHomeBuyer</th>\n",
       "      <th>LoanPurpose</th>\n",
       "      <th>PropertyType</th>\n",
       "      <th>OccType</th>\n",
       "      <th>PropertyState</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>RelocationMortgage</th>\n",
       "      <th>Zip</th>\n",
       "      <th>first_payment_year</th>\n",
       "      <th>loan_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.500</td>\n",
       "      <td>82000</td>\n",
       "      <td>360</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>JPMORGAN CHASE BANK, NA</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>CO</td>\n",
       "      <td>P</td>\n",
       "      <td>CA</td>\n",
       "      <td>FRM</td>\n",
       "      <td>N</td>\n",
       "      <td>927</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.875</td>\n",
       "      <td>165000</td>\n",
       "      <td>360</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>JPMORGAN CHASE BANK, NA</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>CO</td>\n",
       "      <td>P</td>\n",
       "      <td>CA</td>\n",
       "      <td>FRM</td>\n",
       "      <td>N</td>\n",
       "      <td>926</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.375</td>\n",
       "      <td>137000</td>\n",
       "      <td>360</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>JPMORGAN CHASE BANK, NA</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>SF</td>\n",
       "      <td>P</td>\n",
       "      <td>CA</td>\n",
       "      <td>FRM</td>\n",
       "      <td>N</td>\n",
       "      <td>919</td>\n",
       "      <td>2000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.625</td>\n",
       "      <td>113000</td>\n",
       "      <td>180</td>\n",
       "      <td>85.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>AMTRUST BANK</td>\n",
       "      <td>N</td>\n",
       "      <td>R</td>\n",
       "      <td>SF</td>\n",
       "      <td>P</td>\n",
       "      <td>RI</td>\n",
       "      <td>FRM</td>\n",
       "      <td>N</td>\n",
       "      <td>29</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.125</td>\n",
       "      <td>209000</td>\n",
       "      <td>360</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>JPMORGAN CHASE BANK, NA</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>SF</td>\n",
       "      <td>P</td>\n",
       "      <td>CA</td>\n",
       "      <td>FRM</td>\n",
       "      <td>N</td>\n",
       "      <td>926</td>\n",
       "      <td>2000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrInterestRate  OrUnpaidPrinc  OrLoanTerm  OrLTV  OrCLTV  NumBorrowers  DTIRat  CreditScore  NumUnits  MortInsPerc  CoCreditScore  MortInsType  Default Channel               SellerName FTHomeBuyer LoanPurpose PropertyType OccType PropertyState ProductType RelocationMortgage  Zip  first_payment_year  loan_year\n",
       "0           0           8.500          82000         360   97.0   100.0           2.0    59.0        670.0         1         50.0          577.0          1.0        0       B  JPMORGAN CHASE BANK, NA           Y           P           CO       P            CA         FRM                  N  927                2000       2000\n",
       "1           1           7.875         165000         360   97.0   100.0           2.0    38.0        669.0         1         50.0          650.0          1.0        0       B  JPMORGAN CHASE BANK, NA           Y           P           CO       P            CA         FRM                  N  926                2000       2000\n",
       "2           2           8.375         137000         360   97.0   100.0           2.0    31.0        747.0         1         50.0          622.0          1.0        0       B  JPMORGAN CHASE BANK, NA           Y           P           SF       P            CA         FRM                  N  919                2000       1999\n",
       "3           3           7.625         113000         180   85.0    88.0           2.0    18.0        621.0         1         17.0          668.0          1.0        0       C             AMTRUST BANK           N           R           SF       P            RI         FRM                  N   29                1999       1999\n",
       "4           4           8.125         209000         360   97.0   100.0           2.0    64.0        672.0         1         50.0          733.0          1.0        0       B  JPMORGAN CHASE BANK, NA           Y           P           SF       P            CA         FRM                  N  926                2000       1999"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.loc[df['loan_year']==2007]\n",
    "df_new=df_new.drop([\"Unnamed: 0\"], axis=1)\n",
    "df_new[\"MortInsType\"]=df_new[\"MortInsType\"].astype(\"category\")\n",
    "df_new[\"Zip\"]=df_new[\"Zip\"].astype(\"category\")\n",
    "df_new[\"first_payment_year\"]=df_new[\"first_payment_year\"].astype(\"category\")\n",
    "df_new[\"loan_year\"]=df_new[\"loan_year\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12629413038769763"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"Default\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92340, 25)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92340, 982)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.get_dummies(df_new, drop_first=True)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names first\n",
    "names = df_new.columns\n",
    "# Create the Scaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(df_new)\n",
    "scaled_df = pd.DataFrame(df_new, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = scaled_df['Default'].values\n",
    "X = scaled_df.drop(['Default'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix and print it\n",
    "cor = pd.DataFrame(scaled_df).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with output variable\n",
    "cor_target = abs(cor[\"Default\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrInterestRate    0.081594\n",
       "Zip_7             0.010793\n",
       "Zip_121           0.004668\n",
       "Zip_227           0.003032\n",
       "Zip_329           0.025357\n",
       "Zip_438           0.002426\n",
       "Zip_546           0.010635\n",
       "Zip_658           0.002409\n",
       "Zip_776           0.005655\n",
       "Zip_905           0.008656\n",
       "Name: Default, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "cor_target[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrInterestRate                      0.081594\n",
      "OrUnpaidPrinc                       0.067089\n",
      "OrLoanTerm                          0.072374\n",
      "DTIRat                              0.098136\n",
      "CreditScore                         0.140736\n",
      "MortInsPerc                         0.053442\n",
      "CoCreditScore                       0.134827\n",
      "Default                             1.000000\n",
      "SellerName_BANK OF AMERICA, N.A.    0.061761\n",
      "SellerName_OTHER                    0.072229\n",
      "LoanPurpose_P                       0.086688\n",
      "LoanPurpose_R                       0.050651\n",
      "PropertyType_SF                     0.052088\n",
      "PropertyState_AZ                    0.141119\n",
      "PropertyState_CA                    0.114484\n",
      "PropertyState_FL                    0.099971\n",
      "PropertyState_NV                    0.094927\n",
      "PropertyState_TX                    0.055847\n",
      "Zip_850                             0.065583\n",
      "Zip_852                             0.084900\n",
      "Zip_853                             0.084823\n",
      "Zip_890                             0.050815\n",
      "Zip_891                             0.075751\n",
      "Name: Default, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Selecting highly correlated features\n",
    "best_features = cor_target[cor_target > 0.05]\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8553173056097033\n",
      "0.8164609053497942\n",
      "0.6409356725146199\n"
     ]
    }
   ],
   "source": [
    "a_score_knn=accuracy_score(y_test, knn_pred)\n",
    "a_score_dt=accuracy_score(y_test, dt_pred)\n",
    "a_score_lr=accuracy_score(y_test, lr_pred)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(a_score_knn)\n",
    "print(a_score_dt)\n",
    "print(a_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score:\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "$$ F1 = 2 * \\frac{precision * recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07015590200445433\n",
      "0.2394543169987435\n",
      "0.32626188734455014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score_knn = f1_score(y_test, knn_pred)\n",
    "score_dt = f1_score(y_test, dt_pred)\n",
    "score_lr = f1_score(y_test, lr_pred)\n",
    "\n",
    "# Print the f1 scores\n",
    "print(score_knn)\n",
    "print(score_dt)\n",
    "print(score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heterogeneous ensemble \n",
    "* wisedom of the crowd\n",
    "* use fine-tuned models\n",
    "* small amount of estimators\n",
    "* **voting and average**\n",
    "\n",
    "### 1) voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting accuracy-Score: 0.8335282651072125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate the individual models\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "dt = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[('knn', knn), ('dt', dt), ('lr', lr)]\n",
    ")\n",
    "clf_vote.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_vote = clf_vote.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the voting classifier\n",
    "score_vote = accuracy_score(y_test, pred_vote)\n",
    "print('voting accuracy-Score: {:}'.format(score_vote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) averaging (soft voting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate the individual models\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "rf = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_avg = VotingClassifier(\n",
    "    estimators=[('knn', knn), ('dt', dt), ('lr', lr)],\n",
    "    voting='soft',\n",
    "    weights=[2, 1, 1]\n",
    ")\n",
    "\n",
    "\n",
    "clf_avg.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_avg = clf_avg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "acc_avg = accuracy_score(y_test,  pred_avg)\n",
    "print('averaging accuracy: {:}'.format(acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## homogeneous ensemble\n",
    "\n",
    "* use the small model (weak model) \n",
    "* large amount of estimators\n",
    "* **bagging and boosting**\n",
    "* **random forest** is a special case of bagging\n",
    "\n",
    "Condorcet's jury theorm:\n",
    "1) models are independent\n",
    "2) models are slightly better than random guess\n",
    "3) all individual models have similar performance\n",
    "\n",
    "weak model satisfies 2) and 3), bagging algorithm trains individual models using a random subsample for each which guarantee 1). Bootsraping guarantees some of the characteristics of the crowd. Wisedom of the crowd needs to be divers, through using either different algorithms or datasets.\n",
    "\n",
    "Boostrapping requires:\n",
    "* random subsamples\n",
    "* using replacement\n",
    "\n",
    "Boostrapping guarantees:\n",
    "* diverse crowd (different datasets)\n",
    "* indepenent (separately sampled)\n",
    "\n",
    "### why bagging?\n",
    "pro: \n",
    "* bagging usually reduce variance\n",
    "* Overfitting can be avoided by the ensemble itself\n",
    "\n",
    "con:\n",
    "* computational expensive: time and space\n",
    "\n",
    "### bagging with decision tree as the base estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Instantiate the base model\n",
    "clf_dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Build and train the Bagging classifier\n",
    "clf_bag = BaggingClassifier(\n",
    "  base_estimator=clf_dt,\n",
    "  n_estimators=21,\n",
    "  random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "pred = clf_bag.predict(X_test)\n",
    "\n",
    "# Show the accuracy score\n",
    "print('decision tree bagging accuracy score: {:}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest model to obtain importance of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, class_weight=\"balanced\")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred=rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance on the test set to compare\n",
    "print('randomforest accuracy: {:}'.format(accuracy_score(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = df_new.drop('Default', axis=1).columns\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "ncomp = 20\n",
    "sns.barplot(x=feat_labels[indices[:ncomp]], y=importances[indices[:ncomp]], color=sns.xkcd_rgb[\"pale red\"])\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.ylabel('Relative Feature Importance')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## booster\n",
    "gradual learning\n",
    "* iterating learning\n",
    "* dependent estimators\n",
    "* learning different tasks for the same goal\n",
    "* sequential building\n",
    "\n",
    "Possible steps in gradual learning:\n",
    "1. First attempt (initial model)\n",
    "2. Feedback (model evaluation)\n",
    "3. Correct errors (subsequent model)\n",
    "\n",
    "\n",
    "### Adaptive boosting\n",
    "\n",
    "Instances are drawn using a sample distribution\n",
    "* Difcult instances have higher weights\n",
    "* Initialized to be uniform\n",
    "\n",
    "Estimators are combined with a weighted\n",
    "* majority voting\n",
    "* Good estimators are given higher weights\n",
    "\n",
    "Guaranteed to improve\n",
    "\n",
    "Classication and Regression\n",
    "\n",
    "* base_estimator Default: Decision Tree (max_depth=1)\n",
    "* n_estimators Default: 50\n",
    "* learning_rate Default: 1.0\n",
    "* loss default linear (can be change to square or exponential)\n",
    "* Trade-off between n_estimators and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive boosting classifier accuracy:  0.8715183019276587\n"
     ]
    }
   ],
   "source": [
    "# Build and fit a tree-based AdaBoost classifier\n",
    "reg_ada = AdaBoostClassifier(n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "print('Adaptive boosting classifier accuracy:  {:}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "\n",
    "Objective: $$y=f(X) $$\n",
    "1. Initial model (weak estimator): $y = f_{1}(X)$\n",
    "2. New model ts to residuals: $y - f_{1}(X) = f_{2}(X)$\n",
    "3. New additive model: $y = f_{1}(X) + f_{2}(X)$\n",
    "4. Repeat n times or until error is small enough\n",
    "5. Final additive model: $y = f_{1}(X) + f_{2}(X)... + f_{n}(X) = \\Sigma_{1}^{n}f_{X}(X)$\n",
    "\n",
    "**Equivalence to gradient descent**\n",
    "Residual: $$y-f_{i}(X)$$\n",
    "\n",
    "Gradient Descent:\n",
    "$$ loss = \\frac{(f_{i}(X)-y)^2}{2}$$\n",
    "\n",
    "$$gradient = \\frac{\\partial loss}{\\partial f_i} = f_{i}(X)-y$$\n",
    "\n",
    "Residuals = Negative Gradient:\n",
    "$$y-f_{i}(X) = - \\frac{\\partial loss}{\\partial f_i(X)}$$\n",
    "\n",
    "\n",
    "* n_estimators Default: 100\n",
    "* learning_rate Default: 0.1\n",
    "* max_depth Default: 3\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8755035737491877\n"
     ]
    }
   ],
   "source": [
    "# Build and fit a Gradient Boosting classifier\n",
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "clf_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations of gradient boosting\n",
    "* Extreme Gradient Boosting: XGBoost\n",
    "* Light Gradient Boosting Machine: LightGBM\n",
    "* Categorical Boosting: CatBoost\n",
    "\n",
    "\n",
    "### Extreme Gradient Boosting\n",
    "* Optimized for distributed computing\n",
    "* Paralleltraining by nature\n",
    "* Scalable, portable, and accurate\n",
    "\n",
    "### Light Gradient Boosting Machine\n",
    "* Released by Microsoft (2017)\n",
    "* Faster training and more efcient\n",
    "* Lighter in terms of space\n",
    "* Optimized for parallel and GPU processing\n",
    "* Useful for problems with big datasets and constraints of speed or memory\n",
    "\n",
    "### Categorical Boosting\n",
    "* Open sourced by Yandex (April 2017)\n",
    "* Built-in handling of categorical features\n",
    "* Accurate and robust\n",
    "* Fast and scalable\n",
    "* User-friendly API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8738141650422352\n"
     ]
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(max_depth=2, objective=\"reg:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking\n",
    "* alike relay race\n",
    "\n",
    "Very similar to the voting and averaging methods where the whole dataset is used by each model to make prediction, except instead of simply voting or averaging as the combiner to get the final prediction, stacking has a second layer of model which has all the predictions as the input features in addition to the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlxtend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8350010829542993\n"
     ]
    }
   ],
   "source": [
    "# Create the first-layer models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=15, random_state=500)\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Create the second-layer model (meta-model)\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# Create and fit the stacked model\n",
    "clf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb], meta_classifier=clf_lr)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacked model’s performance\n",
    "print(\"Accuracy: {:}\".format(accuracy_score(y_test, clf_stack.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold accuracy: 0.874637210309725\n"
     ]
    }
   ],
   "source": [
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(clf_gbm, X_train, y_train, scoring=\"accuracy\", cv=3)\n",
    "\n",
    "# Print avg. accuracy\n",
    "print(\"3-fold accuracy:\", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation and parameter tuning with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gbm = GradientBoostingClassifier(random_state=500)\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf_gbm__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf_gbm__n_estimators': np.arange(50, 200, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomizedSearchCV\n",
    "randomized_acc = RandomizedSearchCV(estimator=clf_gbm,param_distributions=gbm_param_grid, n_iter=2, scoring=\"accuracy\", cv=2,verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_acc.fit(X_train,y_train)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_acc.best_score_)\n",
    "print(randomized_acc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = randomized_acc.predict(X_test)\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
