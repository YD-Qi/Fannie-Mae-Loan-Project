{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "## data preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "## models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "## ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# import catboost as cb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "## model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "## model evaluation metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acqusition = pd.read_csv('/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data/Acquisition_2007.txt', sep=\"|\",index_col=False)\n",
    "performance = pd.read_csv('/Volumes/Backup Plus/Documents/Data Science/Projects/fannieMae_project/processed_data/Performance_2007.txt', sep=\"|\",index_col=False)\n",
    "df=pd.merge(acqusition, performance, on='LoanID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88073 entries, 0 to 88072\n",
      "Data columns (total 27 columns):\n",
      "LoanID                88073 non-null int64\n",
      "Channel               88073 non-null object\n",
      "SellerName            88073 non-null object\n",
      "OrInterestRate        88073 non-null float64\n",
      "OrUnpaidPrinc         88073 non-null int64\n",
      "OrLoanTerm            88073 non-null int64\n",
      "Origination           88073 non-null object\n",
      "FirstPayment          88073 non-null object\n",
      "OrLTV                 88073 non-null int64\n",
      "OrCLTV                88073 non-null float64\n",
      "NumBorrowers          88073 non-null float64\n",
      "DTIRat                88073 non-null float64\n",
      "CreditScore           88073 non-null float64\n",
      "FTHomeBuyer           88073 non-null object\n",
      "LoanPurpose           88073 non-null object\n",
      "PropertyType          88073 non-null object\n",
      "NumUnits              88073 non-null int64\n",
      "OccType               88073 non-null object\n",
      "PropertyState         88073 non-null object\n",
      "Zip                   88073 non-null int64\n",
      "MortInsPerc           88073 non-null float64\n",
      "ProductType           88073 non-null object\n",
      "CoCreditScore         88073 non-null float64\n",
      "MortInsType           88073 non-null float64\n",
      "RelocationMortgage    88073 non-null object\n",
      "MSA                   11011 non-null float64\n",
      "ForeclosureDate       11011 non-null object\n",
      "dtypes: float64(9), int64(6), object(12)\n",
      "memory usage: 18.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoanID                    0\n",
       "Channel                   0\n",
       "SellerName                0\n",
       "OrInterestRate            0\n",
       "OrUnpaidPrinc             0\n",
       "OrLoanTerm                0\n",
       "Origination               0\n",
       "FirstPayment              0\n",
       "OrLTV                     0\n",
       "OrCLTV                    0\n",
       "NumBorrowers              0\n",
       "DTIRat                    0\n",
       "CreditScore               0\n",
       "FTHomeBuyer               0\n",
       "LoanPurpose               0\n",
       "PropertyType              0\n",
       "NumUnits                  0\n",
       "OccType                   0\n",
       "PropertyState             0\n",
       "Zip                       0\n",
       "MortInsPerc               0\n",
       "ProductType               0\n",
       "CoCreditScore             0\n",
       "MortInsType               0\n",
       "RelocationMortgage        0\n",
       "MSA                   77062\n",
       "ForeclosureDate       77062\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Default']=df['ForeclosureDate']\n",
    "df['Default'].fillna(0, inplace=True)\n",
    "df.loc[df['Default'] != 0, 'Default'] = 1\n",
    "df['Default'] = df['Default'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(s):\n",
    "    \"\"\"\n",
    "    This is an extremely fast approach to datetime parsing.\n",
    "    For large data, the same dates are often repeated. Rather than\n",
    "    re-parse these, we store all unique dates, parse them, and\n",
    "    use a lookup to convert all dates.\n",
    "    \"\"\"\n",
    "    dates = {date:pd.to_datetime(date) for date in s.unique()}\n",
    "    return s.map(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Origination':'loan_date',\n",
    "                          'FirstPayment':'first_payment_date'}, \n",
    "                 inplace=True)\n",
    "\n",
    "df[['loan_date', 'first_payment_date', 'ForeclosureDate']]=df[['loan_date', 'first_payment_date', 'ForeclosureDate']].apply(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_payment_year']=df['first_payment_date'].dt.year\n",
    "df['loan_year']=df['loan_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12502128915785768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Default\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoanID                         int64\n",
       "Channel                       object\n",
       "SellerName                    object\n",
       "OrInterestRate               float64\n",
       "OrUnpaidPrinc                  int64\n",
       "OrLoanTerm                     int64\n",
       "loan_date             datetime64[ns]\n",
       "first_payment_date    datetime64[ns]\n",
       "OrLTV                          int64\n",
       "OrCLTV                       float64\n",
       "NumBorrowers                 float64\n",
       "DTIRat                       float64\n",
       "CreditScore                  float64\n",
       "FTHomeBuyer                   object\n",
       "LoanPurpose                   object\n",
       "PropertyType                  object\n",
       "NumUnits                       int64\n",
       "OccType                       object\n",
       "PropertyState                 object\n",
       "Zip                            int64\n",
       "MortInsPerc                  float64\n",
       "ProductType                   object\n",
       "CoCreditScore                float64\n",
       "MortInsType                  float64\n",
       "RelocationMortgage            object\n",
       "MSA                          float64\n",
       "ForeclosureDate       datetime64[ns]\n",
       "Default                        int64\n",
       "first_payment_year             int64\n",
       "loan_year                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_drop=[\"LoanID\", \"Zip\", \"MSA\", 'first_payment_year', 'loan_year']\n",
    "cat_col_add=['Zip', 'first_payment_year', 'loan_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for numeric columns\n",
    "num_mask = ((df.dtypes == int) | (df.dtypes == float))\n",
    "\n",
    "# Get list of numeric column names\n",
    "num_columns = df.columns[num_mask].tolist()\n",
    "num=df[num_columns]\n",
    "\n",
    "# drop \"categorical\" columns\n",
    "num=num.drop(num_col_drop, axis=1)\n",
    "num=num.fillna(num.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for categorical columns\n",
    "cat_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "cat_columns = df.columns[cat_mask].tolist()\n",
    "\n",
    "cat_columns.extend(cat_col_add)\n",
    "cat=df[cat_columns]\n",
    "cat=cat.astype('category')\n",
    "cat=cat.fillna(cat.mode().iloc[0]) # fill na with most requent category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat num and cat columns and create dummies for cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([num, cat], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88073, 995)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.get_dummies(df_new, drop_first=True)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names first\n",
    "names = df_new.columns\n",
    "# Create the Scaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(df_new)\n",
    "scaled_df = pd.DataFrame(df_new, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = scaled_df['Default'].values\n",
    "X = scaled_df.drop(['Default'], axis=1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8617557563922067\n",
      "0.8241518688405468\n",
      "0.6893137744675053\n"
     ]
    }
   ],
   "source": [
    "a_score_knn=accuracy_score(y_test, knn_pred)\n",
    "a_score_dt=accuracy_score(y_test, dt_pred)\n",
    "a_score_lr=accuracy_score(y_test, lr_pred)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(a_score_knn)\n",
    "print(a_score_dt)\n",
    "print(a_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score:\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "$$ F1 = 2 * \\frac{precision * recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0645359557467732\n",
      "0.2516428295322768\n",
      "0.3513795392054613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score_knn = f1_score(y_test, knn_pred)\n",
    "score_dt = f1_score(y_test, dt_pred)\n",
    "score_lr = f1_score(y_test, lr_pred)\n",
    "\n",
    "# Print the f1 scores\n",
    "print(score_knn)\n",
    "print(score_dt)\n",
    "print(score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heterogeneous ensemble \n",
    "* wisedom of the crowd\n",
    "* use fine-tuned models\n",
    "* small amount of estimators\n",
    "* **voting and average**\n",
    "\n",
    "### 1) voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate the individual models\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "dt = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[('knn', knn), ('dt', dt), ('lr', lr)]\n",
    ")\n",
    "clf_vote.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_vote = clf_vote.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the voting classifier\n",
    "score_vote = accuracy_score(y_test, pred_vote)\n",
    "print('voting accuracy-Score: {:}'.format(score_vote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) averaging (soft voting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate the individual models\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "rf = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_avg = VotingClassifier(\n",
    "    estimators=[('knn', knn), ('dt', dt), ('lr', lr)],\n",
    "    voting='soft',\n",
    "    weights=[2, 1, 1]\n",
    ")\n",
    "\n",
    "\n",
    "clf_avg.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_avg = clf_avg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "acc_avg = accuracy_score(y_test,  pred_avg)\n",
    "print('averaging accuracy: {:}'.format(acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## homogeneous ensemble\n",
    "\n",
    "* use the small model (weak model) \n",
    "* large amount of estimators\n",
    "* **bagging and boosting**\n",
    "* **random forest** is a special case of bagging\n",
    "\n",
    "Condorcet's jury theorm:\n",
    "1) models are independent\n",
    "2) models are slightly better than random guess\n",
    "3) all individual models have similar performance\n",
    "\n",
    "weak model satisfies 2) and 3), bagging algorithm trains individual models using a random subsample for each which guarantee 1). Bootsraping guarantees some of the characteristics of the crowd. Wisedom of the crowd needs to be divers, through using either different algorithms or datasets.\n",
    "\n",
    "Boostrapping requires:\n",
    "* random subsamples\n",
    "* using replacement\n",
    "\n",
    "Boostrapping guarantees:\n",
    "* diverse crowd (different datasets)\n",
    "* indepenent (separately sampled)\n",
    "\n",
    "### why bagging?\n",
    "pro: \n",
    "* bagging usually reduce variance\n",
    "* Overfitting can be avoided by the ensemble itself\n",
    "\n",
    "con:\n",
    "* computational expensive: time and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Instantiate the base model\n",
    "clf_dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Build and train the Bagging classifier\n",
    "clf_bag = BaggingClassifier(\n",
    "  base_estimator=clf_dt,\n",
    "  n_estimators=21,\n",
    "  random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "pred = clf_bag.predict(X_test)\n",
    "\n",
    "# Show the accuracy score\n",
    "print('decision tree bagging accuracy score: {:}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, class_weight=\"balanced\")\n",
    "rf.fit(X_train, y_train)\n",
    "lr_pred=rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance on the test set to compare\n",
    "print('randomforest accuracy: {:}'.format(accuracy_score(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a balanced logistic regression\n",
    "clf_lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Build and fit a bagging classifier\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_lr, max_features=10, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set and show the out-of-bag score\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('logistic classifier accuracy:  {:}'.format(accuracy_score(y_test, pred)))\n",
    "print('OOB-Score: {:}'.format(clf_bag.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## booster\n",
    "gradual learning\n",
    "* iterating learning\n",
    "* dependent estimators\n",
    "* learning different tasks for the same goal\n",
    "* sequential building\n",
    "\n",
    "Possible steps in gradual learning:\n",
    "1. First attempt (initial model)\n",
    "2. Feedback (model evaluation)\n",
    "3. Correct errors (subsequent model)\n",
    "\n",
    "\n",
    "### Adaptive boosting\n",
    "\n",
    "Instances are drawn using a sample distribution\n",
    "* Difcult instances have higher weights\n",
    "* Initialized to be uniform\n",
    "\n",
    "Estimators are combined with a weighted\n",
    "* majority voting\n",
    "* Good estimators are given higher weights\n",
    "\n",
    "Guaranteed to improve\n",
    "\n",
    "Classication and Regression\n",
    "\n",
    "* base_estimator Default: Decision Tree (max_depth=1)\n",
    "* n_estimators Default: 50\n",
    "* learning_rate Default: 1.0\n",
    "* loss default linear (can be change to square or exponential)\n",
    "* Trade-off between n_estimators and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit a tree-based AdaBoost classifier\n",
    "reg_ada = AdaBoostClassifier(n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "print('Adaptive boosting classifier accuracy:  {:}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "\n",
    "Objective: $$y=f(X) $$\n",
    "1. Initial model (weak estimator): $y = f_{1}(X)$\n",
    "2. New model ts to residuals: $y - f_{1}(X) = f_{2}(X)$\n",
    "3. New additive model: $y = f_{1}(X) + f_{2}(X)$\n",
    "4. Repeat n times or until error is small enough\n",
    "5. Final additive model: $y = f_{1}(X) + f_{2}(X)... + f_{n}(X) = \\Sigma_{1}^{n}f_{X}(X)$\n",
    "\n",
    "**Equivalence to gradient descent**\n",
    "Residual: $$y-f_{i}(X)$$\n",
    "\n",
    "Gradient Descent:\n",
    "$$ loss = \\frac{(f_{i}(X)-y)^2}{2}$$\n",
    "\n",
    "$$gradient = \\frac{\\partial loss}{\\partial f_i} = f_{i}(X)-y$$\n",
    "\n",
    "Residuals = Negative Gradient:\n",
    "$$y-f_{i}(X) = - \\frac{\\partial loss}{\\partial f_i(X)}$$\n",
    "\n",
    "\n",
    "* n_estimators Default: 100\n",
    "* learning_rate Default: 0.1\n",
    "* max_depth Default: 3\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit a Gradient Boosting classifier\n",
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "clf_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations of gradient boosting\n",
    "* Extreme Gradient Boosting: XGBoost\n",
    "* Light Gradient Boosting Machine: LightGBM\n",
    "* Categorical Boosting: CatBoost\n",
    "\n",
    "\n",
    "### Extreme Gradient Boosting\n",
    "* Optimized for distributed computing\n",
    "* Paralleltraining by nature\n",
    "* Scalable, portable, and accurate\n",
    "\n",
    "### Light Gradient Boosting Machine\n",
    "* Released by Microsoft (2017)\n",
    "* Faster training and more efcient\n",
    "* Lighter in terms of space\n",
    "* Optimized for parallel and GPU processing\n",
    "* Useful for problems with big datasets and constraints of speed or memory\n",
    "\n",
    "### Categorical Boosting\n",
    "* Open sourced by Yandex (April 2017)\n",
    "* Built-in handling of categorical features\n",
    "* Accurate and robust\n",
    "* Fast and scalable\n",
    "* User-friendly API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier(max_depth=2, objective=\"reg:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking\n",
    "* alike relay race\n",
    "\n",
    "Very similar to the voting and averaging methods where the whole dataset is used by each model to make prediction, except instead of simply voting or averaging as the combiner to get the final prediction, stacking has a second layer of model which has all the predictions as the input features in addition to the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlxtend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first-layer models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=15, random_state=500)\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Create the second-layer model (meta-model)\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# Create and fit the stacked model\n",
    "clf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb], meta_classifier=clf_lr)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacked modelâ€™s performance\n",
    "print(\"Accuracy: {:}\".format(accuracy_score(y_test, clf_stack.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(clf_gbm, X_train, y_train, scoring=\"accuracy\", cv=3)\n",
    "\n",
    "# Print avg. accuracy\n",
    "print(\"3-fold accuracy:\", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation and parameter tuning with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gbm = GradientBoostingClassifier(random_state=500)\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf_gbm__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf_gbm__n_estimators': np.arange(50, 200, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomizedSearchCV\n",
    "randomized_acc = RandomizedSearchCV(estimator=clf_gbm,param_distributions=gbm_param_grid, n_iter=2, scoring=\"accuracy\", cv=2,verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_acc.fit(X_train,y_train)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_acc.best_score_)\n",
    "print(randomized_acc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = randomized_acc.predict(X_test)\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
